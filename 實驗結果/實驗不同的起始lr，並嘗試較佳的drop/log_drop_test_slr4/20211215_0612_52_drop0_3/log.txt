INFO:root:device: cuda
INFO:root:batch_size: 32
INFO:root:drop: 0.3
INFO:root:lr: 0.0001
INFO:root:epochs: 50
INFO:root:Use dataset weight to train.
INFO:root:Dataset weight: {'background': 0.9718910201025287, 'QRCode': 0.028108979897471343}
INFO:root:===================================
INFO:root:epoch: 1, loss: 0.6747225213994857
INFO:root:epoch: 2, loss: 0.6604205095526483
INFO:root:epoch: 3, loss: 0.652118817273644
INFO:root:epoch: 4, loss: 0.6454286717194
INFO:root:epoch: 5, loss: 0.6384640300087288
INFO:root:epoch: 6, loss: 0.6322775188675682
INFO:root:epoch: 7, loss: 0.6234212546003897
INFO:root:epoch: 8, loss: 0.6256255163617336
INFO:root:epoch: 9, loss: 0.6183978352098833
INFO:root:epoch: 10, loss: 0.6206306589571811
INFO:root:epoch: 11, loss: 0.6082460917052204
INFO:root:epoch: 12, loss: 0.604583958386826
INFO:root:epoch: 13, loss: 0.6022717083356657
INFO:root:epoch: 14, loss: 0.5945172145496196
INFO:root:epoch: 15, loss: 0.5963369703698993
INFO:root:epoch: 16, loss: 0.5863196173370653
INFO:root:epoch: 17, loss: 0.5901313659565225
INFO:root:epoch: 18, loss: 0.5821926256789248
INFO:root:epoch: 19, loss: 0.5735216504357358
INFO:root:epoch: 20, loss: 0.5683481519558153
INFO:root:epoch: 21, loss: 0.5615545724746711
INFO:root:epoch: 22, loss: 0.5501407103589007
INFO:root:epoch: 23, loss: 0.5397982019690958
INFO:root:epoch: 24, loss: 0.5188741339833697
INFO:root:epoch: 25, loss: 0.4997486405033433
INFO:root:epoch: 26, loss: 0.46048231583438526
INFO:root:epoch: 27, loss: 0.4374896969378653
INFO:root:epoch: 28, loss: 0.40379735932412725
INFO:root:epoch: 29, loss: 0.38394675387368016
INFO:root:epoch: 30, loss: 0.3703091482582123
INFO:root:epoch: 31, loss: 0.3757417140799552
INFO:root:epoch: 32, loss: 0.34224394787768414
INFO:root:epoch: 33, loss: 0.33514811986333537
INFO:root:epoch: 34, loss: 0.32912222102531413
INFO:root:epoch: 35, loss: 0.32359471970321835
INFO:root:epoch: 36, loss: 0.31251483897525906
INFO:root:epoch: 37, loss: 0.3067840994447195
INFO:root:epoch: 38, loss: 0.3079473823480459
INFO:root:epoch: 39, loss: 0.28938245826827885
INFO:root:epoch: 40, loss: 0.28877490093420444
INFO:root:epoch: 41, loss: 0.296162118083878
INFO:root:epoch: 42, loss: 0.2895594283142582
INFO:root:epoch: 43, loss: 0.29125356422582876
INFO:root:epoch: 44, loss: 0.2864314227864064
INFO:root:epoch: 45, loss: 0.26600514578034423
INFO:root:epoch: 46, loss: 0.27420635613515065
INFO:root:epoch: 47, loss: 0.2768359021909713
INFO:root:epoch: 48, loss: 0.27431824387944753
INFO:root:epoch: 49, loss: 0.27118423881787895
INFO:root:epoch: 50, loss: 0.26178106246087324
INFO:root:lr rate dynamic:
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:  lr: 0.0001
INFO:root:Begin train: 12/15 06:12:52
INFO:root:cost time: 0 days 00:11:58
