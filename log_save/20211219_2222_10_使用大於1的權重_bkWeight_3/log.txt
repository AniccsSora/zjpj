INFO:root:command line: D:/Git/zjpj/main_step1_train.py
INFO:root:============ kay args ============
INFO:root:epochs, 50
INFO:root:lr, 0.001
INFO:root:drop, 0.2
INFO:root:batch_size, 32
INFO:root:reduceLR, True
INFO:root:weight_pt, 
INFO:root:folder_postfix, 使用大於1的權重_bkWeight_3
INFO:root:log_dir, log_save
INFO:root:==================================
INFO:root:device: cuda
INFO:root:batch_size: 32
INFO:root:drop: 0.2
INFO:root:lr: 0.001
INFO:root:epochs: 50
INFO:root:Use dataset weight to train.
INFO:root:Dataset weight: {'background': 0.9602754538728587, 'QRCode': 0.03972454612714131}
INFO:root:===================================
INFO:root:epoch: 1, loss: 0.5304320488358976, lr: 0.001
INFO:root:epoch: 2, loss: 0.4762541896733943, lr: 0.001
INFO:root:epoch: 3, loss: 0.455211398897657, lr: 0.001
INFO:root:epoch: 4, loss: 0.37022419681296337, lr: 0.001
INFO:root:epoch: 5, loss: 0.30614178657219887, lr: 0.001
INFO:root:epoch: 6, loss: 0.29101151796744235, lr: 0.001
INFO:root:epoch: 7, loss: 0.27197334677780943, lr: 0.001
INFO:root:epoch: 8, loss: 0.2604571374978962, lr: 0.001
INFO:root:epoch: 9, loss: 0.23890367112850416, lr: 0.001
INFO:root:epoch: 10, loss: 0.2097745884864085, lr: 0.001
INFO:root:epoch: 11, loss: 0.20316111485645838, lr: 0.001
INFO:root:epoch: 12, loss: 0.1951199458595077, lr: 0.001
INFO:root:epoch: 13, loss: 0.1854333010153867, lr: 0.001
INFO:root:epoch: 14, loss: 0.17149444340768052, lr: 0.001
INFO:root:epoch: 15, loss: 0.16760203640762245, lr: 0.001
INFO:root:epoch: 16, loss: 0.16244768650500652, lr: 0.001
INFO:root:epoch: 17, loss: 0.15442079906898454, lr: 0.001
INFO:root:epoch: 18, loss: 0.15557140924521232, lr: 0.001
INFO:root:epoch: 19, loss: 0.14037387620944114, lr: 0.001
INFO:root:epoch: 20, loss: 0.1372903802243991, lr: 0.001
INFO:root:epoch: 21, loss: 0.1346177181747621, lr: 0.001
INFO:root:epoch: 22, loss: 0.12321068071256018, lr: 0.001
INFO:root:epoch: 23, loss: 0.12414830043874647, lr: 0.001
INFO:root:epoch: 24, loss: 0.12340027731014933, lr: 0.001
INFO:root:epoch: 25, loss: 0.12244821616996747, lr: 0.001
INFO:root:epoch: 26, loss: 0.11785009044228466, lr: 0.001
INFO:root:epoch: 27, loss: 0.11418441197605256, lr: 0.001
INFO:root:epoch: 28, loss: 0.10622784591303652, lr: 0.001
INFO:root:epoch: 29, loss: 0.10784094179624537, lr: 0.001
INFO:root:epoch: 30, loss: 0.10390286508068829, lr: 0.001
INFO:root:epoch: 31, loss: 0.10587972046240056, lr: 0.001
INFO:root:epoch: 32, loss: 0.0990785694797841, lr: 0.001
INFO:root:epoch: 33, loss: 0.10032651399294451, lr: 0.001
INFO:root:epoch: 34, loss: 0.09746562284949212, lr: 0.001
INFO:root:epoch: 35, loss: 0.09909008638412133, lr: 0.001
INFO:root:epoch: 36, loss: 0.09046881465415883, lr: 0.001
INFO:root:epoch: 37, loss: 0.0919486729179664, lr: 0.001
INFO:root:epoch: 38, loss: 0.08803728335332363, lr: 0.001
INFO:root:epoch: 39, loss: 0.08813440644517423, lr: 0.001
INFO:root:epoch: 40, loss: 0.08688987627179719, lr: 0.001
INFO:root:epoch: 41, loss: 0.08973064141916204, lr: 0.001
INFO:root:epoch: 42, loss: 0.08365165348990132, lr: 0.001
INFO:root:epoch: 43, loss: 0.08684404529401273, lr: 0.001
INFO:root:epoch: 44, loss: 0.08886037378441, lr: 0.001
INFO:root:epoch: 45, loss: 0.0840594855003064, lr: 0.001
INFO:root:epoch: 46, loss: 0.08013546333696568, lr: 0.001
INFO:root:epoch: 47, loss: 0.08800205287302813, lr: 0.001
INFO:root:epoch: 48, loss: 0.07889352634226483, lr: 0.001
INFO:root:epoch: 49, loss: 0.07894344410799305, lr: 0.001
INFO:root:epoch: 50, loss: 0.07987214959175915, lr: 0.001
INFO:root:Begin train: 12/19 22:22:10
INFO:root:cost time: 0 days 00:09:57
