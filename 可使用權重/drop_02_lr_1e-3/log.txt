command line: python main_step1_train.py --epochs 50 --lr 1e-3 --log_dir "log_drop_test_slr3" --drop 0.2 --batch_size 32 --reduceLR True --folder_postfix "drop0_2"
INFO:root:device: cuda
INFO:root:batch_size: 32
INFO:root:drop: 0.2
INFO:root:lr: 0.001
INFO:root:epochs: 50
INFO:root:Use dataset weight to train.
INFO:root:Dataset weight: {'background': 0.9718910201025287, 'QRCode': 0.028108979897471343}
INFO:root:===================================
INFO:root:epoch: 1, loss: 0.6558667240496279
INFO:root:epoch: 2, loss: 0.6270130681179286
INFO:root:epoch: 3, loss: 0.5766483669342477
INFO:root:epoch: 4, loss: 0.4849700308224773
INFO:root:epoch: 5, loss: 0.3729389087051757
INFO:root:epoch: 6, loss: 0.33829564793964995
INFO:root:epoch: 7, loss: 0.30608944442898417
INFO:root:epoch: 8, loss: 0.27624989800930405
INFO:root:epoch: 9, loss: 0.2616296974448127
INFO:root:epoch: 10, loss: 0.2437858675549949
INFO:root:epoch: 11, loss: 0.21329957724619197
INFO:root:epoch: 12, loss: 0.20764076379239285
INFO:root:epoch: 13, loss: 0.20604565546892473
INFO:root:epoch: 14, loss: 0.19173921298829094
INFO:root:epoch: 15, loss: 0.1868396830079566
INFO:root:epoch: 16, loss: 0.17026651884355407
INFO:root:epoch: 17, loss: 0.1552270498415318
INFO:root:epoch: 18, loss: 0.1621463650258251
INFO:root:epoch: 19, loss: 0.15459177968809193
INFO:root:epoch: 20, loss: 0.1486906808076133
INFO:root:epoch: 21, loss: 0.1432437291163025
INFO:root:epoch: 22, loss: 0.1361121628553526
INFO:root:epoch: 23, loss: 0.14629632842784462
INFO:root:epoch: 24, loss: 0.12988823573756791
INFO:root:epoch: 25, loss: 0.14309047340650394
INFO:root:epoch: 26, loss: 0.12818406372113259
INFO:root:epoch: 27, loss: 0.13421310486199511
INFO:root:epoch: 28, loss: 0.12964536914823604
INFO:root:epoch: 29, loss: 0.12776728866064002
INFO:root:epoch: 30, loss: 0.1274093652511115
INFO:root:epoch: 31, loss: 0.12585086540814702
INFO:root:epoch: 32, loss: 0.12395824493682114
INFO:root:epoch: 33, loss: 0.1253333482598675
INFO:root:epoch: 34, loss: 0.11846595115013743
INFO:root:epoch: 35, loss: 0.11872058769865527
INFO:root:epoch: 36, loss: 0.12092661186608933
INFO:root:epoch: 37, loss: 0.12170935132162145
INFO:root:epoch: 38, loss: 0.11987810579521331
INFO:root:epoch: 39, loss: 0.12536256038991161
INFO:root:epoch: 40, loss: 0.11557999394429268
INFO:root:epoch: 41, loss: 0.11272171952887773
INFO:root:epoch: 42, loss: 0.11432881419588495
INFO:root:epoch: 43, loss: 0.1042770064425662
INFO:root:epoch: 44, loss: 0.10741717476096745
INFO:root:epoch: 45, loss: 0.1075076760804745
INFO:root:epoch: 46, loss: 0.10604546275109904
INFO:root:epoch: 47, loss: 0.1027429982103085
INFO:root:epoch: 48, loss: 0.10312329057667749
INFO:root:epoch: 49, loss: 0.10849189404879515
INFO:root:epoch: 50, loss: 0.11136868079808245
INFO:root:lr rate dynamic:
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:  lr: 0.001
INFO:root:Begin train: 12/15 04:15:50
INFO:root:cost time: 0 days 00:11:48
